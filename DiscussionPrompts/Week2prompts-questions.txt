Week 3's readings loosely reflect on what kinds of interesting questions we can ask (Davis, 1971) based on the data that we can get (Howison et al., 2011), and the kinds of validity issues that lurk even in data that we might presume to be authoritative (Evans & Schmalensee, 2016).

You can discuss any of the following prompts (but need not reply to all of them); if one has been discussed to death by the time you get to it, try contributing to the conversation around a different one.

 1. Context of data production Evans & Schmalensee make bold claims about the value proposition of government statistical agencies based on what is doubtless an incomplete picture about the Census Bureau's resources and constraints. My experience working with federal employees suggests that they're much more than chronically under-resourced; they're also burdened with extraordinary bureaucracy that limits their ability to fulfill their duties.

Exercise your information-seeking skills and try to figure out one or more of the following: just how much money goes to Census salaries and how many staff does that represent (bonus if you can identify how many are analysts or data managers versus middle management or data collectors); what other duties or agency missions might conflict with turning out the desired information in a timely fashion; and any policies or legal/executive requirements that might constrain staff's ability to meet consumer demand for Census information. What kinds of actions or specific resources (not just money) might make a difference in helping Census Bureau staff meet and exceed public expectations?

2. Asking interesting questions Murray Davis goes into substantial detail on theory and propositions, but a good question is halfway to a theory, so his observations should apply to strong research questions as well. Note that some research questions are fundamentally more descriptive and exploratory--that's OK, but you usually have to do some extra work to argue why we should care about the questions and analysis if there aren't already preconceived expectations among the audience.

Based on the data sets you found and documented for the Information Seeking assignment, can you rephrase any of your research questions to reflect the structure provided by Davis? If so, give us the translation! If not, why do you think the question wouldn't fit this pattern, e.g., is one of the logical categories, like Evolution (p. 340) missing? Finally, what assumptions is the question making about prior knowledge related to the data or phenomenon it describes?

The basic formula for translating the question just requires putting "is" in front of the "what seems to be..." statement and removing it from the middle ("...in reality...") and filling in details specific to the data/context. For example, if you had lots of data on Wikipedia editors' activities, you might ask: "Is what seems to be a single information behavior--editing Wikipedia--in reality composed of assorted heterogeneous behaviors?" (ii, Composition). By the way, the answer is yes: like many complex online communities, Wikipedia editors specialize into multiple roles (Links to an external site.).

3. Threats to validity Howison et al. (2011) outline 10 issues that can interfere with making valid interpretations when using SNA methods on trace data. That's a little methods-specific, so here's why it's relevant: many of you will end up using trace data or found data that may involve similar issues for other types of analyses. Sometimes you'll need to generalize a little to see the connection; for example, Issue 4 can roughly translate to "defining/explaining what your data/measures actually represent in the real world".

For the data sources you found for the Info Seeking assignment (give a one-sentence description of it), which of these threats are not likely to be applicable? How would you try to assess whether the remaining issues are likely to influence your results and interpretation?